# word_embedding

Word embedding is a concept used in natural language processing to describe the representation of words for text processing, which is usually in the form of a real-valued vector that embeds the meaning of the word and determines that words that are near in the vector space will have similar meanings.

word embedding using a Keras model on IMDB Review Dataset

After training, download metadata and vector file and Load in Embedding projector to visualize the data.

Following sample shows relationship between **hated** word with othe words

![Alt text](https://github.com/RohanLone/word_embedding/blob/main/Embedding%20Projector.png)
